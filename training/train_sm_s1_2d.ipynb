{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b9f1afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.7.9: Fast Qwen2 patching. Transformers: 4.53.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3070 Laptop GPU. Num GPUs = 1. Max memory: 8.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastVisionModel\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model, processor = \u001b[43mFastVisionModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 4bit to reduce memory use. False for 16bit LoRA.\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_gradient_checkpointing\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munsloth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# True or \"unsloth\" for long context\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ryan\\miniconda3\\envs\\ftc-lens\\Lib\\site-packages\\unsloth\\models\\loader.py:797\u001b[39m, in \u001b[36mFastModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, return_logits, fullgraph, use_exact_model_name, auto_model, whisper_language, whisper_task, unsloth_force_compile, *args, **kwargs)\u001b[39m\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m auto_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    795\u001b[39m     auto_model = AutoModelForVision2Seq \u001b[38;5;28;01mif\u001b[39;00m is_vlm \u001b[38;5;28;01melse\u001b[39;00m AutoModelForCausalLM\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m model, tokenizer = \u001b[43mFastBaseModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_finetuning\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_finetuning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_types\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m    \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_gradient_checkpointing\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_gradient_checkpointing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[43m    \u001b[49m\u001b[43msupports_sdpa\u001b[49m\u001b[43m     \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msupports_sdpa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhisper_language\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhisper_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    814\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhisper_task\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhisper_task\u001b[49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\n\u001b[32m    815\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    819\u001b[39m     model.resize_token_embeddings(resize_model_vocab)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ryan\\miniconda3\\envs\\ftc-lens\\Lib\\site-packages\\unsloth\\models\\vision.py:430\u001b[39m, in \u001b[36mFastBaseModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, trust_remote_code, model_types, tokenizer_name, auto_model, use_gradient_checkpointing, supports_sdpa, whisper_language, whisper_task, **kwargs)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_forced_float32: torch_dtype = torch.bfloat16\n\u001b[32m    429\u001b[39m raise_handler = RaiseUninitialized()\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m model = \u001b[43mauto_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m              \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m             \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# quantization_config   = bnb_config,\u001b[39;49;00m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# attn_implementation   = attn_implementation,\u001b[39;49;00m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m raise_handler.remove()\n\u001b[32m    441\u001b[39m \u001b[38;5;66;03m# Return old flag\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ryan\\miniconda3\\envs\\ftc-lens\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ryan\\miniconda3\\envs\\ftc-lens\\Lib\\site-packages\\transformers\\modeling_utils.py:311\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    309\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    313\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ryan\\miniconda3\\envs\\ftc-lens\\Lib\\site-packages\\transformers\\modeling_utils.py:4820\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4818\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   4819\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4820\u001b[39m     device_map = \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4822\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   4823\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ryan\\miniconda3\\envs\\ftc-lens\\Lib\\site-packages\\transformers\\modeling_utils.py:1460\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[39m\n\u001b[32m   1457\u001b[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001b[32m   1459\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m         \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1463\u001b[39m     tied_params = find_tied_parameters(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ryan\\miniconda3\\envs\\ftc-lens\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:117\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    118\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    120\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    122\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel\n",
    "\n",
    "model, processor = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-unsloth-bnb-4bit\",\n",
    "    load_in_4bit=True,  # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440f224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=True,  # False if not finetuning vision layers\n",
    "    finetune_language_layers=True,  # False if not finetuning language layers\n",
    "    finetune_attention_modules=True,  # False if not finetuning attention layers\n",
    "    finetune_mlp_modules=True,  # False if not finetuning MLP layers\n",
    "    r=32,  # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha=32,  # Recommended alpha == r at least\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    random_state=7043,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    "    target_modules=\"all-linear\",  # Optional now! Can specify a list if needed\n",
    "    modules_to_save=[\n",
    "        \"lm_head\",\n",
    "        \"embed_tokens\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0879c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1024 examples [00:00, 62224.26 examples/s]\n",
      "Generating test split: 256 examples [00:00, 35967.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "base_url = \"../data_generation/dataset_s1_sm_disk/\"\n",
    "dataset = load_dataset(\n",
    "    \"arrow\",\n",
    "    data_files={\n",
    "        \"train\": base_url + \"train/data-00000-of-00001.arrow\",\n",
    "        \"test\": base_url + \"test/data-00000-of-00001.arrow\"\n",
    "    },\n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d0e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0555619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Identify the GoBilda part shown in this image without further elaboration.\"\n",
    "\n",
    "def conversation_template(sample):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "            ],\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"]}]},\n",
    "    ]\n",
    "    return {\"messages\": conversation}\n",
    "\n",
    "converted_dataset = [conversation_template(sample) for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3775e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows aGoBilda 3060105804432/090A27, which is a part that integrates with the GoBilda door entry system (RDC series). This particular product code corresponds to an access control module used for entering and exiting buildings via the GoBilda home security or access system (RDC series), with a specific model and manufacturer designation mentioned by the end numbers '3060105804432/090A27'. For accurate installation and replacement purposes, always refer to the official user manual provided\n"
     ]
    }
   ],
   "source": [
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "image = dataset[2][\"image\"]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
    "    }\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(processor, skip_prompt=True)\n",
    "result = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    top_k=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ad5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "\n",
    "FastVisionModel.for_training(model)  # Enable for training!\n",
    "torch._dynamo.config.cache_size_limit = 32\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor,\n",
    "    data_collator=UnslothVisionDataCollator(model, processor),  # Must use!\n",
    "    train_dataset=converted_dataset,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        # max_steps=30,\n",
    "        num_train_epochs = 5, # Set this instead of max_steps for full training runs\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=7043,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",  # For Weights and Biases\n",
    "\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns=False,\n",
    "        dataset_text_field=\"\",\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "        max_seq_length=2048,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff0ccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3070 Laptop GPU. Max memory = 8.0 GB.\n",
      "4.137 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed038c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,024 | Num Epochs = 5 | Total steps = 320\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 82,169,856 of 3,836,792,832 (2.14% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [320/320 49:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>7.853600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.474500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.928600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.388800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.837900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.542600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.057300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.042800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.046600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.031500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.028600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.013400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.038400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.034800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.035000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.024400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.022400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.011700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.016400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.018600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.020400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.015400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.022200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.021700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.024200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.014800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.017600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.010900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.018400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.018800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.013500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.018900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.015500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.019600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.012100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.014300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.012800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.007500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.011400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.010800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.010300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.011900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.011100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.009700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.009900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.005200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.011300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.007300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.008600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.009000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.008100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.015700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.004600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.002900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.006500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>-0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.001700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97503719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2989.2084 seconds used for training.\n",
      "49.82 minutes used for training.\n",
      "Peak reserved memory = 4.855 GB.\n",
      "Peak reserved memory for training = 0.718 GB.\n",
      "Peak reserved memory % of max memory = 60.688 %.\n",
      "Peak reserved memory for training % of max memory = 8.975 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedb337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"fl_lora_2d\")  # Local saving\n",
    "processor.save_pretrained(\"fl_lora_2d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52bbf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UChannel1H<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "FastVisionModel.for_inference(model)\n",
    "\n",
    "image = dataset[2][\"image\"]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
    "    }\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(processor, skip_prompt=True)\n",
    "result = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    top_k=64\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ftc-lens",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
